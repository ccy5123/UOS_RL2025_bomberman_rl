# Bomberman RL - Quick Start Guide

## ğŸ“¦ 4ê°œ êµ¬í˜„ ì™„ë£Œ!

### 1ï¸âƒ£ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸ âœ…

**íŒŒì¼**: `benchmark.py`

**ì‚¬ìš©ë²•**:
```bash
# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python benchmark.py

# ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí¬ (ì ì€ ë¼ìš´ë“œ)
python benchmark.py --quick

# íŠ¹ì • ì—ì´ì „íŠ¸ ë¹„êµ
python benchmark.py --agents rule_based_agent random_agent --rounds 100
```

**ê²°ê³¼**:
- ê° ì—ì´ì „íŠ¸ì˜ í‰ê·  ì ìˆ˜, ì½”ì¸ ìˆ˜ì§‘, í‚¬ ìˆ˜, ìì‚´ë¥ , ìƒì¡´ìœ¨
- 1v1 ë§¤ì¹˜ì—… ê²°ê³¼
- ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥

---

### 2ï¸âƒ£ MAPPO êµ¬ì¡° ì„¤ê³„ ë¬¸ì„œ âœ…

**íŒŒì¼**: `MAPPO_DESIGN.md`

**ë‚´ìš©**:
- CTDE (Centralized Training, Decentralized Execution) ì•„í‚¤í…ì²˜
- Actor (Policy Network) + Centralized Critic ì„¤ê³„
- State representation (7-channel CNN)
- Global state for critic
- PPO ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜
- RND (Random Network Distillation) íƒí—˜
- Self-play league ì‹œìŠ¤í…œ
- Reward shaping ì „ëµ
- 8ì£¼ ë¡œë“œë§µ

---

### 3ï¸âƒ£ DQN í”„ë¡œí† íƒ€ì… âœ…

**í´ë”**: `agent_code/dqn_agent/`

**íŒŒì¼ êµ¬ì¡°**:
```
agent_code/dqn_agent/
â”œâ”€â”€ callbacks.py    # DQN ëª¨ë¸, act() í•¨ìˆ˜
â””â”€â”€ train.py        # Experience replay, í•™ìŠµ ë¡œì§
```

**íŠ¹ì§•**:
- 7-channel CNN state representation
- Experience replay buffer
- Target network (stable learning)
- Rule-based guided exploration (Îµ-greedy)
- Reward shaping

**í•™ìŠµ ì‹œì‘**:
```bash
# coin-heavenì—ì„œ í•™ìŠµ ì‹œì‘ (ì‰¬ìš´ ë‚œì´ë„)
python main.py play --my-agent dqn_agent --train 1 --no-gui --n-rounds 1000 --scenario coin-heaven

# classic ëª¨ë“œ í•™ìŠµ
python main.py play --my-agent dqn_agent --train 1 --no-gui --n-rounds 2000 --scenario classic

# í‰ê°€ (GUI)
python main.py play --my-agent dqn_agent --n-rounds 5
```

---

### 4ï¸âƒ£ MAPPO í”„ë¡œí† íƒ€ì… âœ…

**í´ë”**: `agent_code/mappo_agent/`

**íŒŒì¼ êµ¬ì¡°**:
```
agent_code/mappo_agent/
â”œâ”€â”€ config.py       # Hyperparameters
â”œâ”€â”€ features.py     # State extraction (local obs + global state)
â”œâ”€â”€ models.py       # PolicyNetwork, CentralizedValueNetwork, RND
â”œâ”€â”€ callbacks.py    # Agent entry points (setup, act)
â”œâ”€â”€ train.py        # PPO update, GAE, reward shaping
â”œâ”€â”€ checkpoints/    # Saved models
â””â”€â”€ logs/           # Training logs
```

**íŠ¹ì§•**:
- **Centralized Critic**: ëª¨ë“  ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ì‚¬ìš©í•œ ê°€ì¹˜ í‰ê°€
- **Decentralized Actor**: ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ìœ¼ë¡œ í–‰ë™ ì„ íƒ
- **Parameter Sharing**: ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ê°™ì€ ì •ì±… ì‚¬ìš©
- **PPO ì—…ë°ì´íŠ¸**: On-policy, stable learning
- **GAE (Generalized Advantage Estimation)**
- **RND íƒí—˜**: ìƒˆë¡œìš´ ìƒíƒœ íƒí—˜ ë³´ìƒ
- **Reward Shaping**: ìƒì¡´, ì½”ì¸ ì ‘ê·¼, í­íƒ„ íšŒí”¼ ë“±

**í•™ìŠµ ì‹œì‘**:
```bash
# coin-heavenì—ì„œ í•™ìŠµ
python main.py play --my-agent mappo_agent --train 1 --no-gui --n-rounds 1000 --scenario coin-heaven

# Self-play (4ëª… ëª¨ë‘ ê°™ì€ ì—ì´ì „íŠ¸)
python main.py play --agents mappo_agent mappo_agent mappo_agent mappo_agent --train 1 --no-gui --n-rounds 2000

# í‰ê°€
python main.py play --my-agent mappo_agent --n-rounds 5
```

---

## ğŸ¯ ì¤‘ê°„ë°œí‘œ ì¤€ë¹„ (4ì¼ ê³„íš)

### Day 1 (ì˜¤ëŠ˜) âœ…
- [x] ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸
- [x] MAPPO ì„¤ê³„ ë¬¸ì„œ
- [x] DQN í”„ë¡œí† íƒ€ì…
- [x] MAPPO í”„ë¡œí† íƒ€ì…

### Day 2 (ë‚´ì¼)
```bash
# 1. ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • (1ì‹œê°„)
python benchmark.py

# 2. DQN í•™ìŠµ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
nohup python main.py play --my-agent dqn_agent --train 1 --no-gui --n-rounds 5000 --scenario coin-heaven > dqn_train.log 2>&1 &

# 3. MAPPO í•™ìŠµ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
nohup python main.py play --agents mappo_agent mappo_agent mappo_agent mappo_agent --train 1 --no-gui --n-rounds 3000 --scenario coin-heaven > mappo_train.log 2>&1 &

# 4. í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§
tail -f dqn_train.log
tail -f mappo_train.log
```

### Day 3
- í•™ìŠµ ê²°ê³¼ ìˆ˜ì§‘
- ì„±ëŠ¥ ê·¸ë˜í”„ ìƒì„± (í•™ìŠµ ê³¡ì„ )
- ë¬¸ì œì  íŒŒì•… ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ë°œí‘œ ìë£Œ ì´ˆì•ˆ ì‘ì„±

### Day 4
- ë°œí‘œ ìë£Œ ì™„ì„±
- ì‹œì—° ì¤€ë¹„ (GUI ëª¨ë“œ ëŒ€ê²°)
- ë¦¬í—ˆì„¤

---

## ğŸ“Š ë°œí‘œ ìë£Œ êµ¬ì„± (ì¶”ì²œ)

### 1. ë¬¸ì œ ì •ì˜ (5ë¶„)
- ë´„ë²„ë§¨ ê²Œì„ ë£°
- ê°•í™”í•™ìŠµ ë¬¸ì œë¡œì„œì˜ íŠ¹ì„±
- Multi-agent í™˜ê²½ì˜ ë„ì „ê³¼ì œ

### 2. ë² ì´ìŠ¤ë¼ì¸ ë¶„ì„ (5ë¶„)
- rule_based_agent ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
- ê° ì—ì´ì „íŠ¸ ë¹„êµ (í…Œì´ë¸”)
- ëª©í‘œ: rule_basedë¥¼ ë„˜ì–´ì„œê¸°

### 3. ì ‘ê·¼ ë°©ë²•ë¡  (10ë¶„)
#### DQN ì ‘ê·¼
- 7-channel CNN state
- Experience replay
- Rule-guided exploration

#### MAPPO ì ‘ê·¼ (í•µì‹¬!)
- CTDE ì•„í‚¤í…ì²˜ ì„¤ëª…
- Centralized criticì˜ ì´ì 
- PPO ì•Œê³ ë¦¬ì¦˜
- Self-play ì „ëµ

### 4. í˜„ì¬ ì§„í–‰ìƒí™© (5ë¶„)
- êµ¬í˜„ ì™„ë£Œ í•­ëª©
- ì´ˆê¸° í•™ìŠµ ê²°ê³¼ (ê·¸ë˜í”„)
- coin-heavenì—ì„œì˜ ì„±ëŠ¥

### 5. í–¥í›„ ê³„íš (5ë¶„)
- Week 9-10: ê¸°ë³¸ í•™ìŠµ ì™„ì„±
- Week 11-12: Curriculum learning
- Week 13-14: Self-play league
- Week 15-16: ìµœì¢… íŠœë‹

---

## ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

```bash
# 1. ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • (10ë¶„)
python benchmark.py --quick

# 2. DQN ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ (3ë¶„)
python main.py play --my-agent dqn_agent --train 1 --no-gui --n-rounds 10 --scenario coin-heaven

# 3. MAPPO ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ (5ë¶„)
python main.py play --my-agent mappo_agent --train 1 --no-gui --n-rounds 10 --scenario coin-heaven

# 4. ëŒ€ê²° ì‹œì—° (GUI)
python main.py play --agents rule_based_agent dqn_agent mappo_agent random_agent --n-rounds 3
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### PyTorch ì„¤ì¹˜
```bash
pip install torch torchvision
```

### í•™ìŠµì´ ìˆ˜ë ´í•˜ì§€ ì•Šìœ¼ë©´
1. Learning rate ë‚®ì¶”ê¸° (`config.py`)
2. ë” ë§ì€ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ (`EPISODES_PER_UPDATE` ì¦ê°€)
3. Reward scale ì¡°ì •
4. ê°„ë‹¨í•œ ì‹œë‚˜ë¦¬ì˜¤ë¶€í„° ì‹œì‘ (`coin-heaven`)

### ë©”ëª¨ë¦¬ ë¶€ì¡±
1. Batch size ì¤„ì´ê¸°
2. Replay buffer í¬ê¸° ì¤„ì´ê¸°
3. `--n-rounds` ì¤„ì´ê¸°

---

## ğŸ“ˆ ì„±ê³µ ì§€í‘œ

### ì¤‘ê°„ë°œí‘œ ê¸°ì¤€
- [ ] DQNì´ randomë³´ë‹¤ ë‚˜ìŒ (coin-heaven)
- [ ] MAPPOê°€ í•™ìŠµ ê³¡ì„  ë³´ì„
- [ ] ìí­ë¥  < 50%
- [ ] ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ

### ìµœì¢… ëŒ€íšŒ ê¸°ì¤€
- [ ] rule_based 60% ì´ìƒ ìŠ¹ë¥ 
- [ ] í‰ê·  ì ìˆ˜ > 15 (classic 4-player)
- [ ] ìí­ë¥  < 10%
- [ ] ì „ëµì  í­íƒ„ ì„¤ì¹˜

---

## ğŸ’¡ íŒ

1. **ì²˜ìŒì—” coin-heavenìœ¼ë¡œ**: ê°€ì¥ ì‰¬ìš´ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê²€ì¦
2. **ë¡œê·¸ í™•ì¸**: `agent_code/*/logs/` ì—ì„œ í•™ìŠµ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
3. **ì²´í¬í¬ì¸íŠ¸ ì €ì¥**: ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ ì¤‘ë‹¨í•´ë„ ì¬ê°œ ê°€ëŠ¥
4. **Tensorboard ì‚¬ìš©** (ì„ íƒ): í•™ìŠµ ê³¡ì„  ì‹œê°í™”
5. **ë°œí‘œì—ì„  ì†”ì§í•˜ê²Œ**: ì´ˆê¸° ê²°ê³¼ê°€ ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ê´œì°®ìŒ. "ê°œì„  ì¤‘"ì´ ì¤‘ìš”!

---

**ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ! ğŸ‰**

ë‹¤ìŒ ë‹¨ê³„: `python benchmark.py`ë¡œ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì • ì‹œì‘!
